{"cells":[{"cell_type":"markdown","metadata":{},"source":["<center>\n","<img src='https://i.postimg.cc/9XYKq1nD/movie-theatre.png' width=700>\n","</center>\n","\n","# 1. Introduction\n","\n","## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">1.1 Overview</p>\n","\n","Welcome to this notebook on **sentiment analysis** of IMDb **movie reviews**. The task is to classify whether any given review has a **positive or negative** sentiment. \n","\n","We will cover the following:\n","* *Tokenization using spaCy*\n","* *Pre-processing (Stop word removal, Lemmatization, etc)*\n","* *Vectorization using Bag-of-Words and TF-IDF*\n","* *Modelling via Naive Bayes and Logistic Regression*\n","\n","<br>\n","\n","## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">1.2 Libraries</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T04:21:13.869905Z","iopub.status.busy":"2023-11-12T04:21:13.869423Z"},"trusted":true},"outputs":[],"source":["# Core\n","import spacy\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from matplotlib.colors import ListedColormap\n","import seaborn as sns\n","sns.set(style=\"whitegrid\", rc={\"axes.edgecolor\":\"#383838\", \"grid.color\": \"#808080\"}, font_scale=1.6)\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","from bs4 import BeautifulSoup\n","import re\n","\n","# sklearn\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import f1_score, classification_report, ConfusionMatrixDisplay\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","\n","# Color scheme\n","my_colors = [\"#F5DA7F\", \"#EDBA63\", \"#E48E4B\", \"#DE5A36\", \"#E32F1E\", \"#CD1C0F\"]\n","CMAP1 = ListedColormap(my_colors)\n","print(\"Notebook Color Scheme:\")\n","sns.palplot(sns.color_palette(my_colors))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 2. EDA\n","\n","## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">2.1 Load data</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load data into a pandas data frame\n","data = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n","\n","# Preview data frame and shape\n","print(data.shape)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Print first review\n","print(\"Sentiment:\", data.loc[0,\"sentiment\"])\n","print(\"\\nReview:\", data.loc[0,\"review\"])"]},{"cell_type":"markdown","metadata":{},"source":["*Notes:*\n","* The target is **binary** (positive or negative).\n","* There are **50,000 reviews** in the dataset.\n","* The text needs **cleaning** to **remove html tags**."]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">2.2 Distributions</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,4))\n","sns.countplot(data=data, x=\"sentiment\", palette = [my_colors[1],my_colors[4]])\n","plt.title(\"Sentiment distribution\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Print target distribution\n","data[\"sentiment\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate length of each review\n","len_data = pd.concat([data.loc[:,\"review\"].map(lambda x:len(x)),data.loc[:,\"sentiment\"]],axis=1)\n","\n","# Plot distribution\n","plt.figure(figsize=(12,5))\n","sns.histplot(data=len_data, x=\"review\", hue=\"sentiment\", palette = [my_colors[1],my_colors[4]], binwidth=100, kde=True)\n","plt.title(\"Length of reviews\")\n","plt.xlabel(\"Length\")\n","plt.xlim([0,6000])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Average positive review length:\", len_data.loc[len_data[\"sentiment\"]==\"positive\",\"review\"].mean())\n","print(\"Average negative review length:\", len_data.loc[len_data[\"sentiment\"]==\"negative\",\"review\"].mean())"]},{"cell_type":"markdown","metadata":{},"source":["*Notes:*\n","* The target is completely **balanced** (50% positive, 50% negative).\n","* Negative reviews tend to be marginally **longer** on average."]},{"cell_type":"markdown","metadata":{},"source":["# 3. Pre-processing\n","\n","## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">3.1 Clean data</p>\n","\n","Let's clean the data by **removing html tags**, which can be done by using the **BeautifulSoup library**. We'll also **remove urls** and **special characters**."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def remove_html_tags(text):\n","    soup = BeautifulSoup(text,\"html.parser\")\n","    return soup.get_text()\n","\n","def remove_urls(text):\n","    return re.sub(\"http\\S+\",\"\",text)\n","\n","def remove_special_characters(text):\n","    return re.sub(\"[^A-Za-z0-9 ]+\",\"\",text)\n","\n","def clean_text(text):\n","    text = remove_html_tags(text)\n","    text = remove_urls(text)\n","    text = remove_special_characters(text)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data cleaning example\n","example = \" Kaggle<br /><br /> was first launched in 2010. (Website here->[https://www.kaggle.com/])\"\n","print(\"Before:\", example)\n","print(\"After:\", clean_text(example))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Clean entire dataset\n","data[\"review\"] = data[\"review\"].apply(clean_text)"]},{"cell_type":"markdown","metadata":{},"source":["Compare this now cleaned **first review**, with the uncleaned version at the top of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Print first review after data cleaning\n","data.loc[0,\"review\"]"]},{"cell_type":"markdown","metadata":{},"source":["Note: It is ok that we have **removed punctuation** since we won't be using sequential models. Instead we are only interested in the **frequency** and relative frequency of words between documents."]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">3.2 Encode target</p>\n","\n","We need to convert the target to **integers**. We map positive to 1 and negative to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data[\"sentiment\"] = data[\"sentiment\"].replace(\"positive\",1).replace(\"negative\",0)"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">3.3 Split data</p>\n","\n","We need to create a **test set** to be able to **evaluate our models**."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Split features and labels\n","X = data[\"review\"]\n","y = data[\"sentiment\"]\n","\n","# Split train and test set\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0,stratify=y)\n","\n","# Print train target distribution\n","print(y_train.value_counts())"]},{"cell_type":"markdown","metadata":{},"source":["This method ensures that the train and test sets **remain balanced**."]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">3.4 Tokenization</p>\n","\n","We're going to build a custom tokenizer using **spacy**. We'll **remove stop words** (i.e. common words with little meaning) and **apply lemmatization** (i.e. convert words to dictionary form), which are common NLP preprocessing techniques. Furthermore, we only keep tokens if they are an **adjective, proper noun or verb** to reduce the size of the vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Disable named-entity recognition and parsing to save time\n","unwanted_pipes = [\"ner\", \"parser\"]\n","\n","# Custom tokenizer using spacy\n","def custom_tokenizer(doc):\n","    with nlp.disable_pipes(*unwanted_pipes):\n","        return [t.lemma_ for t in nlp(doc) if not t.is_stop and not t.is_space and t.pos_ in [\"ADJ\",\"NOUN\",\"VERB\"]]"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">3.5 Vectorization</p>\n","\n","We have to convert **variable length strings** of text to **fixed length numerical vectors** to able to train machine learning models on this data. There are two popular approaches for this.\n","\n","1. **Bow-of-Words**; simply counts how many times each word appears in a document."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Bag-of-Words vectorizer\n","vectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n","\n","# Fit and transform train data\n","X_train_bow = vectorizer.fit_transform(X_train)\n","\n","# Transform test data\n","X_test_bow = vectorizer.transform(X_test)\n","\n","# Print vocab size\n","print(\"BoW vocabulary size:\", X_train_bow.shape[1])"]},{"cell_type":"markdown","metadata":{},"source":["2. **TF-IDF**; multiples term frequency with inverse document frequency. This puts more weight on words that don't appear in many documents. "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":false,"trusted":true},"outputs":[],"source":["%%time\n","\n","# TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n","\n","# Fit and transform train data\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","\n","# Transform test data\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","# Print vocab size\n","print(\"TF-IDF vocabulary size:\", X_train_tfidf.shape[1])"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Modelling with Naive Bayes\n","\n","## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">4.1 Using BoW vectors</p>\n","\n","Our first model will be **Naive Bayes** on the **Bag-of-Words (BoW) vectorized data**. We'll also tune the additive smoothing hyperparameter `alpha` using grid search. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Parameters to tune\n","grid = {\"alpha\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n","\n","# Define model\n","clf = MultinomialNB()\n","\n","# Define grid search\n","nb_bow = GridSearchCV(clf, param_grid=grid, scoring=\"f1_macro\", n_jobs=-1, cv=5, verbose=5)\n","\n","# Train model using grid search\n","nb_bow.fit(X_train_bow, y_train)\n","\n","# Print best value of alpha\n","print(\"Best parameters:\", nb_bow.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make predictions on test set\n","nb_bow_preds = nb_bow.predict(X_test_bow)\n","\n","# Print classification report\n","print(classification_report(y_test, nb_bow_preds, target_names=[\"negative\",\"positive\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Figure size\n","fig, ax = plt.subplots(figsize=(6, 6))\n","ax.grid(False)\n","\n","# Create the confusion matrix\n","disp = ConfusionMatrixDisplay.from_estimator(nb_bow, X_test_bow, y_test, cmap=\"Reds\", display_labels=[\"negative\",\"positive\"], xticks_rotation=\"vertical\", ax=ax)"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">4.2 Using TF-IDF vectors</p>\n","\n","Now we'll apply **Naive Bayes** on the **TF-IDF vectorized data**. Let's see how it compares to using Bag-of-Words."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Parameters to tune\n","grid = {\"alpha\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}\n","\n","# Define model\n","clf = MultinomialNB()\n","\n","# Define grid search\n","nb_tfidf = GridSearchCV(clf, param_grid=grid, scoring=\"f1_macro\", n_jobs=-1, cv=5, verbose=5)\n","\n","# Train model using grid search\n","nb_tfidf.fit(X_train_tfidf, y_train)\n","\n","# Print best value of alpha\n","print(\"Best parameters:\", nb_tfidf.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make predictions on test set\n","nb_tfidf_preds = nb_tfidf.predict(X_test_tfidf)\n","\n","# Print classification report\n","print(classification_report(y_test, nb_tfidf_preds, target_names=[\"negative\",\"positive\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Figure size\n","fig, ax = plt.subplots(figsize=(6, 6))\n","ax.grid(False)\n","\n","# Create the confusion matrix\n","disp = ConfusionMatrixDisplay.from_estimator(nb_tfidf, X_test_tfidf, y_test, cmap=\"Reds\", display_labels=[\"negative\",\"positive\"], xticks_rotation=\"vertical\", ax=ax)"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Modelling with Logistic Regression\n","\n","## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">5.1 Using BoW vectors</p>\n","\n","Now we'll use a **Logistic Regression** model on the **Bag-of-Words vectorized data** to see how it compares to Naive Bayes. We'll also tune the regularization hyperparameters of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Parameters to tune\n","grid = {\"penalty\": [\"l1\",\"l2\"], \"solver\": [\"liblinear\"], \"C\": [0.25, 0.5, 0.75, 1, 1.25, 1.5]}\n","\n","# Define model\n","clf = LogisticRegression()\n","\n","# Define grid search\n","lr_bow = GridSearchCV(clf, param_grid=grid, scoring=\"f1_macro\", n_jobs=-1, cv=5, verbose=5)\n","\n","# Train model using grid search\n","lr_bow.fit(X_train_bow, y_train)\n","\n","# Print best value of alpha\n","print(\"Best parameters:\", lr_bow.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make predictions on test set\n","lr_bow_preds = lr_bow.predict(X_test_bow)\n","\n","# Print classification report\n","print(classification_report(y_test, lr_bow_preds, target_names=[\"negative\",\"positive\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Figure size\n","fig, ax = plt.subplots(figsize=(6, 6))\n","ax.grid(False)\n","\n","# Create the confusion matrix\n","disp = ConfusionMatrixDisplay.from_estimator(lr_bow, X_test_bow, y_test, cmap=\"Reds\", display_labels=[\"negative\",\"positive\"], xticks_rotation=\"vertical\", ax=ax)"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#E32F1E; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #d51e0f;\">5.2 Using TF-IDF vectors</p>\n","\n","Finally, let's use **Logistic Regression** on the **TF-IDF vectorized data**."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Parameters to tune\n","grid = {\"penalty\": [\"l1\",\"l2\"], \"solver\": [\"liblinear\"], \"C\": [0.25, 0.5, 0.75, 1, 1.25, 1.5]}\n","\n","# Define model\n","clf = LogisticRegression()\n","\n","# Define grid search\n","lr_tfidf = GridSearchCV(clf, param_grid=grid, scoring=\"f1_macro\", n_jobs=-1, cv=5, verbose=5)\n","\n","# Train model using grid search\n","lr_tfidf.fit(X_train_tfidf, y_train)\n","\n","# Print best value of alpha\n","print(\"Best parameters:\", lr_tfidf.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make predictions on test set\n","lr_tfidf_preds = lr_tfidf.predict(X_test_tfidf)\n","\n","# Print classification report\n","print(classification_report(y_test, lr_tfidf_preds, target_names=[\"negative\",\"positive\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Figure size\n","fig, ax = plt.subplots(figsize=(6, 6))\n","ax.grid(False)\n","\n","# Create the confusion matrix\n","disp = ConfusionMatrixDisplay.from_estimator(lr_tfidf, X_test_tfidf, y_test, cmap=\"Reds\", display_labels=[\"negative\",\"positive\"], xticks_rotation=\"vertical\", ax=ax)"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Conclusion\n","\n","We can summarize the findings as follows:\n","\n","* TF-IDF did **marginally better** than Bag-of-Words; although the difference was less than 1% in F-1 score. \n","* Logistic Regression **outperformed** Naive Bayes by at least 2% in F-1 score in both cases. \n","* Logistic Regression may have performed better because of its ability to **regularize** on the large vocabulary."]},{"cell_type":"markdown","metadata":{},"source":["**References:**\n","* [Sentiment Analysis of IMDB Movie Reviews](https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews)\n","* [NLP5 - Text Classification with Naive Bayes](https://www.kaggle.com/code/samuelcortinhas/nlp5-text-classification-with-naive-bayes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Thanks for reading!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
