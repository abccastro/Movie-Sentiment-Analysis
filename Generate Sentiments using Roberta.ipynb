{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Generate Sentiments using RoBERTa__\n",
    "\n",
    "The code aims to split movie reviews into smaller files, analyze and assign sentiments to each review individually, and subsequently merge the divided movie files into a single consolidated file.\n",
    "\n",
    "- The goal is to reduce the impact of a single record failure, preventing it from affecting the entire set of records.\n",
    "- By dividing records for each member, the code increases processing speed, optimizing the efficiency of the overall operation.\n",
    "\n",
    "__Pre-requisites:__\n",
    "- Get the assigned zipped file from https://mylambton.sharepoint.com/:f:/r/sites/NLPandSocialMediaAnalytics/Shared%20Documents/General/Split%20Dataset%20(Movie%20Reviews)?csf=1&web=1&e=1EaRaL\n",
    "- Create a directory named 'split_dataset' within the 'dataset' folder. Transfer all files into the newly created 'split_dataset' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import contractions\n",
    "import os\n",
    "import text_preprocessing as tp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'reviews'\n",
    "source_data_directory = \"./dataset/\"\n",
    "split_dataset_directory = './dataset/split_dataset/'                # contains the split files\n",
    "done_split_dataset_directory = './dataset/split_dataset/done/'      # contains split files that were done processing\n",
    "sentiment_directory = './dataset/split_dataset/sentiment/'          # contains files with sentiments\n",
    "done_sentiment_directory = './dataset/split_dataset/sentiment/done' # output directory of the combined file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the movie review records into several smaller files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save_csv(input_file, output_prefix, chunk_size):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Save each chunk to a separate CSV file\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_file = f\"{output_prefix}_{i + 1}.csv\"\n",
    "        chunk.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(chunk)} records to {output_file}\")\n",
    "\n",
    "\n",
    "if not os.path.exists(split_dataset_directory):\n",
    "    os.makedirs(split_dataset_directory)\n",
    "\n",
    "input_csv = source_data_directory + filename + \".csv\"\n",
    "output_prefix = split_dataset_directory + filename   \n",
    "chunk_size = 100     \n",
    "\n",
    "split_and_save_csv(input_csv, output_prefix, chunk_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning sentiment labels on each movie review record using RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading source the file: ./dataset/split_dataset/reviews_4201.csv\n",
      "Parallel processing of movie reviews...\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Initialize the English stop words list\n",
    "list_of_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def data_preprocessing(text):\n",
    "    text = tp.remove_email_address(text)\n",
    "    text = tp.remove_hyperlink(text)\n",
    "    text = tp.replace_whitespace(text)\n",
    "    text = tp.remove_stopwords(text, list_of_stopwords)\n",
    "    return text\n",
    "\n",
    "\n",
    "def polarity_score_roberta(data):\n",
    "    # Do basic data pre-processing\n",
    "    data_preprocessing(data)\n",
    "\n",
    "    # Specify the maximum sequence length\n",
    "    max_length = 512  # Adjust this based on the model's maximum sequence length\n",
    "\n",
    "    # Tokenize and truncate/pad the input text\n",
    "    encoded_text = tokenizer(data, return_tensors='tf', max_length=max_length, truncation=True, padding=True)\n",
    "    \n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    scores_dict = {\n",
    "        \"roberta_neg\": scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    \n",
    "    return scores_dict\n",
    "\n",
    "# Define the function to generate labels\n",
    "def generate_roberta_labels(data, positive_threshold=0.5, negative_threshold=0.5):\n",
    "    # Get roberta scores\n",
    "    scores = polarity_score_roberta(data)\n",
    "    roberta_neg, roberta_neu, roberta_pos = scores['roberta_neg'], scores['roberta_neu'], scores['roberta_pos']\n",
    "\n",
    "    sentiment_results_dict = {'positive': roberta_pos, \n",
    "                              'negative': roberta_neg, \n",
    "                              'neutral': roberta_neu}\n",
    "\n",
    "    highest_sentiment = max(sentiment_results_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "    if highest_sentiment[0] == 'positive':    \n",
    "        if highest_sentiment[1] >= 0.8:        \n",
    "            return \"Strongly Positive\" \n",
    "        else:        \n",
    "            return \"Positive\"\n",
    "    elif highest_sentiment[0] == 'negative':    \n",
    "        if highest_sentiment[1] >= 0.8:        \n",
    "            return \"Strongly Negative\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "    df['roberta_sentiment'] = df['review_detail'].apply(lambda x : generate_roberta_labels(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def parallel_processing(df, func, num_threads=3):\n",
    "    # NOTE: Change the number of threads depending on device's CPU core\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunks = np.array_split(df, num_threads)\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(func, chunks))\n",
    "\n",
    "    # Concatenate the results\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def list_files(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            files.append(filepath)\n",
    "    return files\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    files_in_directory = list_files(split_dataset_directory)\n",
    "\n",
    "    # Create 'done' directory inside the 'split_dataset\n",
    "    if not os.path.exists(done_split_dataset_directory):\n",
    "        print(\"Creating 'done' directory...\")\n",
    "        os.makedirs(done_split_dataset_directory)\n",
    "\n",
    "    # Create 'sentiment' directory inside the 'split_dataset\n",
    "    if not os.path.exists(sentiment_directory):\n",
    "        print(\"Creating 'sentiment' directory...\")\n",
    "        os.makedirs(sentiment_directory)\n",
    "\n",
    "    for input_filepath in files_in_directory:\n",
    "\n",
    "        try:\n",
    "            base_filename = os.path.basename(input_filepath)\n",
    "            no_ext_base_filename = os.path.splitext(base_filename)[0]\n",
    "            output_filepath = sentiment_directory + no_ext_base_filename + \"_sentiment.csv\"\n",
    "\n",
    "            print(f\"Reading source the file: {input_filepath}\")\n",
    "            movie_reviews_df = pd.read_csv(input_filepath)\n",
    "            \n",
    "            print(\"Parallel processing of movie reviews...\")\n",
    "            movie_reviews_df = parallel_processing(movie_reviews_df, process_dataframe)\n",
    "\n",
    "            # Save to csv file\n",
    "            movie_reviews_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "            print(f\"\\nSuccessfully saved the file with sentiments in {output_filepath}\")\n",
    "\n",
    "            # Move the file that was processed to the done folder\n",
    "            destination_file = os.path.join(done_split_dataset_directory, base_filename)\n",
    "            os.rename(input_filepath, destination_file)\n",
    "            \n",
    "            print(f\"\\nMove source file to done folder: {destination_file}\")\n",
    "        except Exception as err:\n",
    "            print(f\"ERROR: {err}\")\n",
    "            print(f\"File: {input_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging several movie review files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(sentiment_directory) if file.endswith('.csv')]\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        try:            \n",
    "            file_path = os.path.join(sentiment_directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "\n",
    "            # Move the file that was processed to the done folder\n",
    "            destination_file = os.path.join(done_sentiment_directory, file)\n",
    "            os.rename(file_path, destination_file)\n",
    "            \n",
    "            print(f\"Move source file to done folder: {destination_file}\")\n",
    "        except Exception as err:\n",
    "            print(f\"ERROR: {err}\")\n",
    "            print(f\"Filename: {file_path}\")\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_data.to_csv(os.path.join(sentiment_directory, 'combined_reviews.csv'), index=False)\n",
    "\n",
    "    print(\"CSV files combined successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
